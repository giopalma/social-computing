{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Secondo progetto di Social Computing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# importaimo il file .csv usando la libreria Pandas\n",
    "df = pd.read_csv(\"group_5-Palma-Sacchet-Sagliocca.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load utils.py\n",
    "import json\n",
    "import os\n",
    "\n",
    "\n",
    "def serealize_json(folder, filename, data):\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder,exist_ok=True)\n",
    "    with open(f\"{folder}/{filename}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data,f,ensure_ascii=False,indent=4)\n",
    "        f.close()\n",
    "    print(f\"Data serialized to path: {folder}/{filename}.json\")\n",
    "\n",
    "def read_json(path):\n",
    "    try:\n",
    "        if os.path.exists(path):\n",
    "            with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "                data = json.load(file)\n",
    "            return data\n",
    "    except ValueError:\n",
    "        print(\"Path not found, check the correctness of the path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funziona che permette di crere un elemento di un HIT (document)\n",
    "def document_factory(row,human:bool):\n",
    "    document = {\n",
    "        'id': f'{row['id']}',\n",
    "        'statement': row['statement'],\n",
    "        'explanation': row['explanation_human'] if human else row['explanation_model'],\n",
    "        'label': row['label']\n",
    "    }\n",
    "    return document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from nanoid import generate\n",
    "# Alfabeto utilizzato da nanodi per generare gli ID dei HIT\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "\n",
    "# Funzione per generare un HIT\n",
    "def hit_factory(index:int):\n",
    "\n",
    "    documents = []\n",
    "\n",
    "    # L'obiettivo è selezionare d'apprima una coppia di righe tra le tre, queste avranno le spiegazioni generate\n",
    "    model_df = df.sample(2)\n",
    "    # E successivamente selezionare un altra riga sempre tra le tre, ques'ultima avrà la spiegazione umana\n",
    "    human_df = df.sample()\n",
    "\n",
    "    # Aggiungo i due elementi con explanation_model all'HIT\n",
    "    for _, row in model_df.iterrows():\n",
    "        document = document_factory(row,False)\n",
    "        documents.append(document)\n",
    "    # Aggiungo l'elemento con explanation_human all'HIT\n",
    "    for _, row in human_df.iterrows():\n",
    "        document = document_factory(row,True)\n",
    "        documents.append(document)\n",
    "\n",
    "    # Mescolo casualmente gli elementi dell'HIT\n",
    "    random.shuffle(documents)\n",
    "\n",
    "    # Creo l'HIT\n",
    "    hit = {\n",
    "        'unit_id': f'unit_{index}',\n",
    "        'token_input': generate(alphabet,size=11), # Genera ID di 11 caratteri\n",
    "        'token_output': generate(alphabet,size=11), # Genera ID di 11 caratteri\n",
    "        'documents_number': len(documents),\n",
    "        'documents': documents\n",
    "    }\n",
    "    \n",
    "    return hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data serialized to path: ./hits.json\n"
     ]
    }
   ],
   "source": [
    "# Inizializzo l'array dove mettero gli HITs\n",
    "hits = []\n",
    "\n",
    "# Genero i 12 HITs\n",
    "for i in range(12):\n",
    "    hit = hit_factory(i)\n",
    "    hits.append(hit)\n",
    "\n",
    "serealize_json(\".\",\"hits\",hits)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisi dei dati"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "from itertools import combinations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punto 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_agreement(v1,v2):\n",
    "    if v1 != v2:\n",
    "        return (100 - (abs(v1 - v2))/(v1 + v2)*100)\n",
    "    else:\n",
    "        return 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_44299/53874010.py:43: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result = result.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_44299/53874010.py:43: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result = result.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_44299/53874010.py:43: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result = result.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_44299/53874010.py:43: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result = result.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_44299/53874010.py:43: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result = result.append(row, ignore_index=True)\n",
      "/tmp/ipykernel_44299/53874010.py:43: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  result = result.append(row, ignore_index=True)\n"
     ]
    }
   ],
   "source": [
    "# L'obiettivo è quello di raggruppare il dataset per explanation, successivamente per ogni explanation selezionare i worker che l'hanno fatto e tra essi creare le coppie\n",
    "# Successivamente tra queste coppie calcolare il Percent Agreement per truthfulness-1 e truthfulness-2\n",
    "# Calcolare la media e andare avanti\n",
    "df = pd.read_csv(\"csv/workers_answers.csv\")\n",
    "df = df[['doc_explanation', 'worker_id', 'doc_truthfulness-1_value', 'doc_truthfulness-2_value']]\n",
    "\n",
    "groups = df.groupby(by=['doc_explanation'])\n",
    "result = pd.DataFrame()\n",
    "for name, group in groups:\n",
    "    workers = list(group['worker_id'])\n",
    "    group = group.drop_duplicates()\n",
    "    group = group.dropna()\n",
    "    group = group.set_index('worker_id')\n",
    "    # Creo le combinazioni tra worker\n",
    "    worker_couples = list(combinations(workers,2))\n",
    "    # Imposto i due percent agreement a 0 in modo da fare la media alla fine\n",
    "    total_percent_agreement1 = 0\n",
    "    total_percent_agreement2 = 0\n",
    "    for couple in worker_couples:\n",
    "        # Estraggo i valori\n",
    "        worker1_truthfulness1 = float(group['doc_truthfulness-1_value'].loc[couple[0]])\n",
    "        worker2_truthfulness1 = float(group['doc_truthfulness-1_value'].loc[couple[1]])\n",
    "        # Estraggo i valori\n",
    "        worker1_truthfulness2 = float(group['doc_truthfulness-2_value'].loc[couple[0]])\n",
    "        worker2_truthfulness2 = float(group['doc_truthfulness-2_value'].loc[couple[1]])\n",
    "        # Trovo i due percent_agreement\n",
    "        percent_agreement1 = percent_agreement(worker1_truthfulness1,worker2_truthfulness1)\n",
    "        percent_agreement2 = percent_agreement(worker1_truthfulness2,worker2_truthfulness2)\n",
    "        # Sommo i percent agreement calcolati al totale\n",
    "        total_percent_agreement1 += percent_agreement1\n",
    "        total_percent_agreement2 += percent_agreement2\n",
    "    n = len(worker_couples)\n",
    "    mean_percent_agreement1 = total_percent_agreement1/n\n",
    "    mean_percent_agreement2 = total_percent_agreement2/n\n",
    "    row = {\n",
    "        'explanation': name,\n",
    "        'percent_agreement_truthfulness1': mean_percent_agreement1,\n",
    "        'percent_agreement_truthfulness2': mean_percent_agreement2\n",
    "    }\n",
    "    result = result.append(row, ignore_index=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Punto 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explanation_mapping(text, explanation_series):\n",
    "    for i, explanation in explanation_series.iteritems():\n",
    "        ratio = SequenceMatcher(None, str(text), str(explanation)).ratio()\n",
    "        if (ratio > 0.9):\n",
    "            return i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_21426/1404189285.py:3: FutureWarning: The series.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  explanation_df = explanation_df['explanation_human'].append(explanation_df['explanation_model']).reset_index(drop=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Saw II sold over 3 million units, however it does not say whether it sold them on DVD.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explanation_df = pd.read_csv(\"group_5-Palma-Sacchet-Sagliocca.csv\")\n",
    "explanation_df = explanation_df[['explanation_human', 'explanation_model']]\n",
    "explanation_df = explanation_df['explanation_human'].append(explanation_df['explanation_model']).reset_index(drop=True)\n",
    "explanation_df[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'[nan] not in index'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m note_df[\u001b[39m'\u001b[39m\u001b[39mexplanation_id\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m note_df[\u001b[39m'\u001b[39m\u001b[39mnote_text\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m text: explanation_mapping(text, explanation_df))\n\u001b[1;32m     10\u001b[0m \u001b[39m# Sostutuisco note_text con la spiegazione corretta in base\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m note_df[\u001b[39m'\u001b[39m\u001b[39mnote_text\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m explanation_df[note_df[\u001b[39m'\u001b[39;49m\u001b[39mexplanation_id\u001b[39;49m\u001b[39m'\u001b[39;49m]]\n\u001b[1;32m     12\u001b[0m \u001b[39m# Conto il numero di parole sottolineate e quelle totali\u001b[39;00m\n\u001b[1;32m     13\u001b[0m note_df[\u001b[39m'\u001b[39m\u001b[39mnote_text_raw_count\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m note_df[\u001b[39m'\u001b[39m\u001b[39mnote_text_raw\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: \u001b[39mlen\u001b[39m(re\u001b[39m.\u001b[39mfindall(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mstr\u001b[39m(x))))\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/core/series.py:984\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    981\u001b[0m     key \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(key, dtype\u001b[39m=\u001b[39m\u001b[39mbool\u001b[39m)\n\u001b[1;32m    982\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_values(key)\n\u001b[0;32m--> 984\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_with(key)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/core/series.py:1024\u001b[0m, in \u001b[0;36mSeries._get_with\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1021\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39miloc[key]\n\u001b[1;32m   1023\u001b[0m \u001b[39m# handle the dup indexing case GH#4246\u001b[39;00m\n\u001b[0;32m-> 1024\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloc[key]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/core/indexing.py:967\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    964\u001b[0m axis \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxis \u001b[39mor\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[1;32m    966\u001b[0m maybe_callable \u001b[39m=\u001b[39m com\u001b[39m.\u001b[39mapply_if_callable(key, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj)\n\u001b[0;32m--> 967\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_axis(maybe_callable, axis\u001b[39m=\u001b[39;49maxis)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1191\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(key, \u001b[39m\"\u001b[39m\u001b[39mndim\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m key\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1189\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCannot index with multidimensional key\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1191\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem_iterable(key, axis\u001b[39m=\u001b[39;49maxis)\n\u001b[1;32m   1193\u001b[0m \u001b[39m# nested tuple slicing\u001b[39;00m\n\u001b[1;32m   1194\u001b[0m \u001b[39mif\u001b[39;00m is_nested_tuple(key, labels):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1132\u001b[0m, in \u001b[0;36m_LocIndexer._getitem_iterable\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1129\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_key(key, axis)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# A collection of keys\u001b[39;00m\n\u001b[0;32m-> 1132\u001b[0m keyarr, indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_listlike_indexer(key, axis)\n\u001b[1;32m   1133\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_reindex_with_indexers(\n\u001b[1;32m   1134\u001b[0m     {axis: [keyarr, indexer]}, copy\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, allow_dups\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/core/indexing.py:1327\u001b[0m, in \u001b[0;36m_LocIndexer._get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1324\u001b[0m ax \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_axis(axis)\n\u001b[1;32m   1325\u001b[0m axis_name \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39m_get_axis_name(axis)\n\u001b[0;32m-> 1327\u001b[0m keyarr, indexer \u001b[39m=\u001b[39m ax\u001b[39m.\u001b[39;49m_get_indexer_strict(key, axis_name)\n\u001b[1;32m   1329\u001b[0m \u001b[39mreturn\u001b[39;00m keyarr, indexer\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py:5782\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5779\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   5780\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 5782\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[1;32m   5784\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[1;32m   5785\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[1;32m   5786\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py:5845\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5842\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   5844\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[0;32m-> 5845\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: '[nan] not in index'"
     ]
    }
   ],
   "source": [
    "# Carico il file csv da dove ricavo i dati\n",
    "note_df = pd.read_csv(\"csv/workers_notes.csv\")\n",
    "note_df['note_text_left'] = note_df['note_text_left'].fillna(\"\")\n",
    "note_df['note_text_right'] = note_df['note_text_right'].fillna(\"\")\n",
    "note_df['note_text'] = note_df['note_text_left'].astype(str) + note_df['note_text_raw'].astype(str) + note_df['note_text_right'].astype(str)\n",
    "# Converto le frasi in numeri (contando il numero di parole). Utilizzo regex per non considerare la punteggiatura\n",
    "note_df['note_text_raw_count'] = note_df['note_text_raw'].apply(lambda x: len(re.findall(r'\\w+', str(x))))\n",
    "note_df['note_text_count'] = note_df['note_text'].apply(lambda x: len(re.findall(r'\\w+', str(x))))\n",
    "note_df['percentual'] = (note_df['note_text_raw_count'] / note_df['note_text_count']) * 100\n",
    "note_df['explanation_id'] = note_df['note_text'].apply(lambda text: explanation_mapping(text, explanation_df))\n",
    "note_df = note_df.groupby(by='explanation_id').mean().reset_index()\n",
    "note_df = pd.DataFrame({\n",
    "    'explanation': explanation_df,\n",
    "    'percentual_mean': note_df['percentual']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataframe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Filtro solo le colonne che mi interessano\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m note_df \u001b[39m=\u001b[39m dataframe[[\u001b[39m'\u001b[39m\u001b[39mnote_text_raw\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mnote_text_left\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mnote_text_right\u001b[39m\u001b[39m'\u001b[39m]]\n\u001b[1;32m      3\u001b[0m \u001b[39m# Converto le frasi in numeri (contando il numero di parole). Utilizzo regex per non considerare la punteggiatura\u001b[39;00m\n\u001b[1;32m      4\u001b[0m note_df_num \u001b[39m=\u001b[39m note_df\u001b[39m.\u001b[39mapplymap(\u001b[39mlambda\u001b[39;00m x: \u001b[39mlen\u001b[39m(re\u001b[39m.\u001b[39mfindall(\u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mw+\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mstr\u001b[39m(x))))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataframe' is not defined"
     ]
    }
   ],
   "source": [
    "# Filtro solo le colonne che mi interessano\n",
    "note_df = dataframe[['note_text_raw', 'note_text_left', 'note_text_right']]\n",
    "# Converto le frasi in numeri (contando il numero di parole). Utilizzo regex per non considerare la punteggiatura\n",
    "note_df_num = note_df.applymap(lambda x: len(re.findall(r'\\w+', str(x))))\n",
    "# Aggiungo una nuova colonna con il totale delle parole\n",
    "note_df_num['note_text_total'] = str(note_df_num['note_text_left']) + str(note_df_num['note_text_raw']) + str(note_df_num['note_text_right'])\n",
    "# Aggiungo una nuova colonna con il valore percentuale\n",
    "note_df_num['percentual'] = (note_df_num['note_text_raw'] / note_df_num['note_text_total']) * 100\n",
    "note_df_num['worker_id'] = dataframe['worker_id']\n",
    "note_df_num"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Punto 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paid</th>\n",
       "      <th>try_last</th>\n",
       "      <th>try_current</th>\n",
       "      <th>dimension_index</th>\n",
       "      <th>timestamp_start</th>\n",
       "      <th>selection_index</th>\n",
       "      <th>selection_value</th>\n",
       "      <th>selection_timestamp</th>\n",
       "      <th>selection_time_elapsed</th>\n",
       "      <th>timestamp_end</th>\n",
       "      <th>document_index</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1520</th>\n",
       "      <td>6.200000</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>6.200000</td>\n",
       "      <td>5.600000</td>\n",
       "      <td>1.037430e+10</td>\n",
       "      <td>9.800000</td>\n",
       "      <td>125.800000</td>\n",
       "      <td>1.037430e+10</td>\n",
       "      <td>443.357200</td>\n",
       "      <td>1.037430e+10</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25842</th>\n",
       "      <td>5.333333</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>5.333333</td>\n",
       "      <td>5.111111</td>\n",
       "      <td>8.924056e+09</td>\n",
       "      <td>12.333333</td>\n",
       "      <td>224.777778</td>\n",
       "      <td>8.924057e+09</td>\n",
       "      <td>301.741667</td>\n",
       "      <td>8.924057e+09</td>\n",
       "      <td>6.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41947</th>\n",
       "      <td>4.500000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>4.500000</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>7.529652e+09</td>\n",
       "      <td>5.583333</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>7.529653e+09</td>\n",
       "      <td>304.114833</td>\n",
       "      <td>7.529653e+09</td>\n",
       "      <td>3.833333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 paid  try_last  try_current  dimension_index  \\\n",
       "document_id                                                     \n",
       "1520         6.200000  6.200000     6.200000         5.600000   \n",
       "25842        5.333333  5.333333     5.333333         5.111111   \n",
       "41947        4.500000  4.500000     4.500000         4.333333   \n",
       "\n",
       "             timestamp_start  selection_index  selection_value  \\\n",
       "document_id                                                      \n",
       "1520            1.037430e+10         9.800000       125.800000   \n",
       "25842           8.924056e+09        12.333333       224.777778   \n",
       "41947           7.529652e+09         5.583333       134.000000   \n",
       "\n",
       "             selection_timestamp  selection_time_elapsed  timestamp_end  \\\n",
       "document_id                                                               \n",
       "1520                1.037430e+10              443.357200   1.037430e+10   \n",
       "25842               8.924057e+09              301.741667   8.924057e+09   \n",
       "41947               7.529653e+09              304.114833   7.529653e+09   \n",
       "\n",
       "             document_index  \n",
       "document_id                  \n",
       "1520               8.000000  \n",
       "25842              6.000000  \n",
       "41947              3.833333  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv(\"csv/workers_dimensions_selection.csv\")\n",
    "\n",
    "# Utilizzo la funzione groupby pre raggruppare per dimension, dopo di chè \n",
    "dataframe.groupby(by=[\"worker_id\", \"document_id\"]).sum().groupby(by=\"document_id\").mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b3c36198a59ad4403c7a551e6916c61a78e0010e1ee5e0850198f49740cf93cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
